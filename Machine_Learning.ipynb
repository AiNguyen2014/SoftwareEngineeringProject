{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AiNguyen2014/SoftwareEngineeringProject/blob/main/Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6vjok6wr44w"
      },
      "source": [
        "# Lưu ý\n",
        "\n",
        "# Mình sẽ đánh giá essemble của train bằng silhoutte Và đánh giá external giữa independent với actual. Internal chỉ dùng để đánh giá train thôi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CpUc-vhjV_BB",
        "outputId": "ed343923-7fb7-41fb-ff95-8714d7015a62"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3926928862.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mTARGET_FOLDER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Project Machine Learning\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "TARGET_FOLDER = \"Project Machine Learning\"\n",
        "BASE_PATH = None\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content/drive/MyDrive\"):\n",
        "    if TARGET_FOLDER in dirs:\n",
        "        BASE_PATH = os.path.join(root, TARGET_FOLDER)\n",
        "        break\n",
        "\n",
        "if BASE_PATH is None:\n",
        "    raise FileNotFoundError(\" Không tìm thấy thư mục Project Machine Learning\")\n",
        "\n",
        "print(\" Dùng thư mục:\", BASE_PATH)\n",
        "print(\" File trong thư mục:\", os.listdir(BASE_PATH))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anN6FqI4j8qi"
      },
      "source": [
        "# Setup môi trường"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_Sg5yyHkkAei"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import warnings\n",
        "from sklearn.decomposition import PCA\n",
        "import joblib\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5eWmaRCjsz6"
      },
      "source": [
        "# Tải dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "chnILDrVjuoi"
      },
      "outputs": [],
      "source": [
        "# Train & Independent: gene là index\n",
        "df_train = pd.read_csv(\n",
        "    os.path.join(BASE_PATH, \"data_set_ALL_AML_train.csv\"),\n",
        "    index_col=0\n",
        ")\n",
        "\n",
        "df_test = pd.read_csv(\n",
        "    os.path.join(BASE_PATH, \"data_set_ALL_AML_independent.csv\"),\n",
        "    index_col=0\n",
        ")\n",
        "\n",
        "# actual.csv: chỉ chứa nhãn → KHÔNG dùng index_col\n",
        "df_actual = pd.read_csv(\n",
        "    os.path.join(BASE_PATH, \"actual.csv\")\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", df_train.shape)\n",
        "print(\"Independent shape:\", df_test.shape)\n",
        "print(\"Actual shape:\", df_actual.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2dCKSbNlv-7"
      },
      "source": [
        "# Tiền Xử Lý Dữ Liệu (Clean + Transpose + Z-score + PCA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hWuo9_TZlyXC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# =============================\n",
        "# CLEAN GENE EXPRESSION\n",
        "# =============================\n",
        "def clean_gene_expression(df):\n",
        "    drop_cols = [\n",
        "        col for col in df.columns\n",
        "        if \"Gene Description\" in col\n",
        "        or \"Gene Accession Number\" in col\n",
        "        or \"call\" in col.lower()\n",
        "    ]\n",
        "    return df.drop(columns=drop_cols, errors=\"ignore\")\n",
        "\n",
        "\n",
        "df_train_clean = clean_gene_expression(df_train)\n",
        "df_test_clean  = clean_gene_expression(df_test)\n",
        "\n",
        "# =============================\n",
        "# TRANSPOSE (samples × genes)\n",
        "# =============================\n",
        "X_train_raw = df_train_clean.T   # (38, genes)\n",
        "X_test_raw  = df_test_clean.T    # (34, genes)\n",
        "\n",
        "# =============================\n",
        "# GỘP TOÀN BỘ 72 MẪU\n",
        "# =============================\n",
        "X_all_raw = pd.concat([X_train_raw, X_test_raw], axis=0)\n",
        "\n",
        "print(\"Total samples after merge:\", X_all_raw.shape)\n",
        "\n",
        "# =============================\n",
        "# Z-SCORE NORMALIZATION (ALL)\n",
        "# =============================\n",
        "scaler = StandardScaler()\n",
        "X_all_scaled = scaler.fit_transform(X_all_raw)\n",
        "\n",
        "print(\"Scaled shape:\", X_all_scaled.shape)\n",
        "\n",
        "# =============================\n",
        "# FEATURE SELECTION (SUPERVISED – CHỈ ĐỂ GIẢM CHIỀU)\n",
        "# =============================\n",
        "N_GENES = 200\n",
        "\n",
        "# Tính variance từng gene\n",
        "gene_variances = np.var(X_all_scaled, axis=0)\n",
        "\n",
        "# Chọn top N_GENES variance cao nhất\n",
        "top_gene_idx = np.argsort(gene_variances)[-N_GENES:]\n",
        "X_all_fs = X_all_scaled[:, top_gene_idx]\n",
        "\n",
        "print(f\"Selected top {N_GENES} genes (unsupervised)\")\n",
        "print(\"After FS shape:\", X_all_fs.shape)\n",
        "\n",
        "# =============================\n",
        "# SAVE FINAL DATASET (KHÔNG PCA)\n",
        "# =============================\n",
        "# Lấy tên cột từ top gene index để dễ hiểu (nếu muốn)\n",
        "gene_cols = [f\"Gene_{i+1}\" for i in top_gene_idx]\n",
        "\n",
        "X_all_out = pd.DataFrame(\n",
        "    X_all_fs,\n",
        "    index=X_all_raw.index,\n",
        "    columns=gene_cols\n",
        ")\n",
        "X_all_out.insert(0, \"Sample_ID\", X_all_out.index)\n",
        "\n",
        "X_all_out.to_csv(\n",
        "    os.path.join(BASE_PATH, \"data_processed_72.csv\"),\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(\"\\n=== PREPROCESSING COMPLETE (NO PCA) ===\")\n",
        "print(\"Saved to: data_processed_72.csv\", X_all_fs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RVCUbPflyyC"
      },
      "source": [
        "# BASE MODELS - K-MEANS++"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INbwIlRAXUbV"
      },
      "source": [
        "## Import thư viện cần thiết"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ftjq1h7GXZkG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import silhouette_score\n",
        "import seaborn as sns\n",
        "from scipy.spatial.distance import cdist\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Thiết lập style cho biểu đồ\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py2_i783Xcpk"
      },
      "source": [
        "## Load dữ liệu đã chuẩn hóa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1EtOuoVJXdNc"
      },
      "outputs": [],
      "source": [
        "# Đọc dữ liệu train đã được chuẩn hóa\n",
        "try:\n",
        "    train_path = os.path.join(BASE_PATH, 'data_processed_72.csv')\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    print(f\"Đã tải dữ liệu Train từ: {train_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi tải file: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Bỏ cột định danh không phải feature nếu tồn tại\n",
        "if 'Sample_ID' in train_df.columns:\n",
        "    train_df = train_df.drop(columns=['Sample_ID'])\n",
        "\n",
        "# Chuyển thành numpy array\n",
        "X_train = train_df.values  # hoặc train_df.to_numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ymb6pwbrXlfH"
      },
      "outputs": [],
      "source": [
        "# Chuyển đổi DataFrame sang numpy array để tính toán\n",
        "X = train_df.values\n",
        "print(f\"\\nMảng dữ liệu X có shape: {X.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XwPIkrd-D3v"
      },
      "source": [
        "## Định nghĩa các hàm cần dùng trong K means++"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MLD3crdG9vnB"
      },
      "outputs": [],
      "source": [
        "# ═══ HÀM TÍNH KHOẢNG CÁCH ═══\n",
        "def euclidean_distance(point1, point2):\n",
        "    \"\"\"Tính khoảng cách Euclidean giữa hai điểm\"\"\"\n",
        "    return np.sqrt(np.sum((point1 - point2) ** 2))\n",
        "\n",
        "def calculate_distances(X, centroids):\n",
        "    \"\"\"\n",
        "    Tính ma trận khoảng cách từ TẤT CẢ điểm đến TẤT CẢ centroids\n",
        "    Output: (n_samples, n_clusters)\n",
        "    \"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    distances = np.zeros((n_samples, n_clusters))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_clusters):\n",
        "            distances[i, j] = euclidean_distance(X[i], centroids[j])\n",
        "\n",
        "    return distances\n",
        "\n",
        "# ═══ HÀM TÍNH SILHOUETTE SCORE (dùng sklearn) ═══\n",
        "def calculate_silhouette_score(X, labels):\n",
        "    \"\"\"Tính Silhouette Score bằng sklearn.metrics.silhouette_score\"\"\"\n",
        "    return silhouette_score(X, labels)\n",
        "print(\"Đã định nghĩa hàm tiện ích: khoảng cách, silhouette score (sklearn)\")\n",
        "\n",
        "\n",
        "# ═══ BƯỚC 2: KHỞI TẠO CENTROIDS (K-MEANS++) ═══\n",
        "def initialize_centroids_kmeans_plusplus(X, k, random_state=None):\n",
        "\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    n_samples = X.shape[0]\n",
        "    centroids = [X[np.random.randint(0, n_samples)]]\n",
        "\n",
        "    for _ in range(1, k):\n",
        "        distances = np.array([min([euclidean_distance(x, c) for c in centroids]) for x in X])\n",
        "        probabilities = distances ** 2\n",
        "        probabilities /= probabilities.sum()\n",
        "        next_centroid_idx = np.random.choice(n_samples, p=probabilities)\n",
        "        centroids.append(X[next_centroid_idx])\n",
        "\n",
        "    return np.array(centroids)\n",
        "\n",
        "# ═══ BƯỚC 3: GÁN CỤM ═══\n",
        "def assign_clusters(X, centroids):\n",
        "\n",
        "    distances = calculate_distances(X, centroids)\n",
        "    return np.argmin(distances, axis=1)\n",
        "\n",
        "# ═══ BƯỚC 4: CẬP NHẬT CENTROIDS ═══\n",
        "def update_centroids(X, labels, k):\n",
        "\n",
        "    n_features = X.shape[1]\n",
        "    centroids = np.zeros((k, n_features))\n",
        "\n",
        "    for i in range(k):\n",
        "        cluster_points = X[labels == i]\n",
        "        if len(cluster_points) > 0:\n",
        "            centroids[i] = cluster_points.mean(axis=0)\n",
        "        else:\n",
        "            centroids[i] = X[np.random.randint(0, X.shape[0])]\n",
        "\n",
        "    return centroids\n",
        "\n",
        "# ═══ HÀM K-MEANS++ HOÀN CHỈNH ═══\n",
        "def kmeans_plusplus(X, k, max_iters=100, tol=1e-4, random_state=None, verbose=False):\n",
        "\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    # BƯỚC 2: Khởi tạo centroids\n",
        "    centroids = initialize_centroids_kmeans_plusplus(X, k, random_state)\n",
        "\n",
        "    history = {\n",
        "        'iterations': 0,\n",
        "        'shifts': [],\n",
        "        'converged': False\n",
        "    }\n",
        "\n",
        "    for iteration in range(max_iters):\n",
        "        # BƯỚC 3: Gán các điểm vào cụm\n",
        "        labels = assign_clusters(X, centroids)\n",
        "\n",
        "        # Lưu centroid cũ\n",
        "        old_centroids = centroids.copy()\n",
        "\n",
        "        # BƯỚC 4: Cập nhật centroids\n",
        "        centroids = update_centroids(X, labels, k)\n",
        "\n",
        "        # BƯỚC 5: Kiểm tra hội tụ\n",
        "        centroid_shift = np.sum([euclidean_distance(old_centroids[i], centroids[i])\n",
        "                                for i in range(k)])\n",
        "\n",
        "        history['shifts'].append(centroid_shift)\n",
        "        history['iterations'] = iteration + 1\n",
        "\n",
        "        if verbose and (iteration + 1) % 10 == 0 or iteration == 0:\n",
        "            print(f\"  Iteration {iteration + 1}: Shift={centroid_shift:.2e}\")\n",
        "\n",
        "        if centroid_shift < tol:\n",
        "            history['converged'] = True\n",
        "            if verbose:\n",
        "                print(f\"  Hội tụ tại iteration {iteration + 1}\")\n",
        "            break\n",
        "\n",
        "    return labels, centroids, history\n",
        "\n",
        "print(\"Đã định nghĩa các hàm hỗ trợ (Bước 2, 3, 4, 5)\")\n",
        "print(\"  (Sẽ sử dụng ở BƯỚC 1: Đánh giá K, và BƯỚC 2-5: Huấn luyện)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwpk3JBVavmw"
      },
      "source": [
        "## BƯỚC 1: ĐÁNH GIÁ & LỰA CHỌN SỐ CỤM K TỐI ƯU\n",
        "**Thực hiện:** Chạy K-means++ với K từ 2 đến 10, so sánh Silhouette Score, chọn K tốt nhất"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ytDRsaZHaxOy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "═══════════════════════════════════════════════════════════════════════════════\n",
        " BƯỚC 1: ĐÁNH GIÁ - CHẠY K-MEANS++ VỚI K = 2 ĐẾN 10\n",
        "═══════════════════════════════════════════════════════════════════════════════\n",
        "MỤC TIÊU: Tìm K tối ưu bằng Silhouette Score\n",
        "MỤC ĐÍCH: Xác định giá trị K tốt nhất trước khi huấn luyện chi tiết (Bước 2-5)\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BƯỚC 1: ĐÁNH GIÁ LỰA CHỌN SỐ CỤM K TỐI ƯU\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nChạy K-means++ với K từ 2 đến 10 trên dữ liệu train_scaled.csv\")\n",
        "print(\"So sánh Silhouette Score để lựa chọn K tốt nhất\\n\")\n",
        "\n",
        "k_values = range(2, 11)\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"[K={k:2d}]\", end=\" \")\n",
        "\n",
        "    # Chạy K-MEANS++ với k cụm\n",
        "    labels, centroids, history = kmeans_plusplus(X, k, max_iters=300, random_state=42)\n",
        "\n",
        "    # Tính Silhouette Score\n",
        "    silhouette = calculate_silhouette_score(X, labels)\n",
        "    silhouette_scores.append(silhouette)\n",
        "\n",
        "    print(f\"Silhouette = {silhouette:7.4f} | Iterations = {history['iterations']}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "\n",
        "# Tìm k tốt nhất\n",
        "best_k_idx = np.argmax(silhouette_scores)\n",
        "best_k = list(k_values)[best_k_idx]\n",
        "best_silhouette = silhouette_scores[best_k_idx]\n",
        "\n",
        "print(f\"\\nKẾT QUẢ ĐÁNH GIÁ:\")\n",
        "print(f\"   K có Silhouette Score cao nhất: K = {best_k}\")\n",
        "print(f\"   Silhouette Score: {best_silhouette:.4f}\")\n",
        "print(f\"\\n   SẼ CHỌN: K = {best_k} cho bước huấn luyện tiếp theo\")\n",
        "print(f\"{'='*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LqoeYDl48p4g"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "═══════════════════════════════════════════════════════════════════════════════\n",
        "    BƯỚC 2-5: HUẤN LUYỆN MÔ HÌNH K-MEANS++ CHI TIẾT\n",
        "═══════════════════════════════════════════════════════════════════════════════\n",
        "MỤC TIÊU: Sử dụng K tốt nhất từ BƯỚC 1 để huấn luyện mô hình chi tiết\n",
        "MỤC ĐÍCH: Theo dõi từng iteration - gán → cập nhật → kiểm tra hội tụ\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BƯỚC 2-5: HUẤN LUYỆN MÔ HÌNH K-MEANS++ CHI TIẾT\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nSỐ CỤM ĐƯỢC CHỌN: K = {best_k}\")\n",
        "print(f\"  (Dựa trên Silhouette Score cao nhất = {best_silhouette:.4f})\\n\")\n",
        "\n",
        "# Chạy K-means với K tốt nhất, verbose để thấy quá trình\n",
        "print(f\"{'─'*80}\")\n",
        "print(\"Chạy K-means++ với K = {} trên dữ liệu train_scaled.csv\".format(best_k))\n",
        "print(f\"{'─'*80}\\n\")\n",
        "\n",
        "labels_final, centroids_final, training_history = kmeans_plusplus(\n",
        "    X, best_k, max_iters=100, tol=1e-4, random_state=42, verbose=True\n",
        ")\n",
        "\n",
        "print(f\"\\n{'─'*80}\")\n",
        "if training_history['converged']:\n",
        "    print(f\"THUẬT TOÁN HỘI TỤ sau {training_history['iterations']} iterations\")\n",
        "else:\n",
        "    print(f\"ĐẠT SỐ ITERATIONS TỐI ĐA ({training_history['iterations']})\")\n",
        "print(f\"{'─'*80}\\n\")\n",
        "\n",
        "# Lưu lịch sử\n",
        "history_centroid_shifts = training_history['shifts']\n",
        "iteration_count = training_history['iterations']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma-Jr4pa8rqJ"
      },
      "source": [
        "## ĐÁNH GIÁ KẾT QUẢ MÔ HÌNH TRÊN DỮ LIỆU TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mlNClQLc8seZ"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"THỐNG KÊ CHI TIẾT VỀ KẾT QUẢ HỌC\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Tính chỉ số cuối cùng\n",
        "final_silhouette = calculate_silhouette_score(X, labels_final)\n",
        "\n",
        "print(f\"\\nTHÔNG TIN CHUNG:\")\n",
        "print(f\"   - Dữ liệu: train_scaled.csv\")\n",
        "print(f\"   - Số mẫu (samples): {X.shape[0]}\")\n",
        "print(f\"   - Số features: {X.shape[1]}\")\n",
        "print(f\"   - Số cụm (K): {best_k}\")\n",
        "\n",
        "print(f\"\\nCHỈ SỐ ĐÁNH GIÁ:\")\n",
        "print(f\"   - Silhouette Score: {final_silhouette:.4f}\")\n",
        "\n",
        "print(f\"\\nTHÔNG TIN HỌC:\")\n",
        "print(f\"   - Số iterations: {iteration_count}\")\n",
        "print(f\"   - Trạng thái: {'Hội tụ' if training_history['converged'] else 'Max iterations'}\")\n",
        "\n",
        "print(f\"\\nPHÂN BỐ CÁC CỤM:\")\n",
        "for cluster_id in range(best_k):\n",
        "    count = np.sum(labels_final == cluster_id)\n",
        "    percentage = (count / len(labels_final)) * 100\n",
        "    print(f\"   - Cụm {cluster_id}: {count:4d} điểm ({percentage:5.1f}%)\")\n",
        "\n",
        "print(f\"\\nTHÔNG TIN CENTROIDS:\")\n",
        "for i, centroid in enumerate(centroids_final):\n",
        "    print(f\"   Centroid {i}:\")\n",
        "    print(f\"     - Giá trị trung bình: {centroid.mean():.4f}\")\n",
        "    print(f\"     - Độ lệch chuẩn: {centroid.std():.4f}\")\n",
        "    print(f\"     - Range: [{centroid.min():.4f}, {centroid.max():.4f}]\")\n",
        "\n",
        "print(f\"\\n{'='*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrjitwZvl-yq"
      },
      "source": [
        "# BASE MODELS - HIERARCHICAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0k7OKrmHPZ1"
      },
      "source": [
        "# **Xây dựng mô hình (Training) và đánh giá nội bộ (chưa đụng đến tập Test hay so sánh với thực tế)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cgvtG6GjP-_t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import silhouette_score\n",
        "import os\n",
        "\n",
        "# ============================================================\n",
        "# PHẦN 1: BASE LEARNER - HIERARCHICAL CLUSTERING\n",
        "# ============================================================\n",
        "\n",
        "class HierarchicalClustering:\n",
        "    def __init__(self, n_clusters=2, linkage='ward'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.linkage = linkage\n",
        "        self.labels_ = None\n",
        "\n",
        "    def _euclidean_distance(self, point1, point2):\n",
        "        return np.sqrt(np.sum((point1 - point2) ** 2))\n",
        "\n",
        "    def _compute_distance_matrix(self, X):\n",
        "        n = X.shape[0]\n",
        "        dist_matrix = np.zeros((n, n))\n",
        "        for i in range(n):\n",
        "            for j in range(i+1, n):\n",
        "                dist = self._euclidean_distance(X[i], X[j])\n",
        "                dist_matrix[i, j] = dist\n",
        "                dist_matrix[j, i] = dist\n",
        "        return dist_matrix\n",
        "\n",
        "    # --- Linkage Functions ---\n",
        "    def _single_linkage(self, c1_idxs, c2_idxs, dist_matrix):\n",
        "        min_dist = float('inf')\n",
        "        for i in c1_idxs:\n",
        "            for j in c2_idxs:\n",
        "                if dist_matrix[i, j] < min_dist: min_dist = dist_matrix[i, j]\n",
        "        return min_dist\n",
        "\n",
        "    def _complete_linkage(self, c1_idxs, c2_idxs, dist_matrix):\n",
        "        max_dist = 0\n",
        "        for i in c1_idxs:\n",
        "            for j in c2_idxs:\n",
        "                if dist_matrix[i, j] > max_dist: max_dist = dist_matrix[i, j]\n",
        "        return max_dist\n",
        "\n",
        "    def _ward_linkage(self, c1_idxs, c2_idxs, X):\n",
        "        m1 = np.mean(X[c1_idxs], axis=0)\n",
        "        m2 = np.mean(X[c2_idxs], axis=0)\n",
        "        n1, n2 = len(c1_idxs), len(c2_idxs)\n",
        "        return np.sqrt((2 * n1 * n2) / (n1 + n2)) * self._euclidean_distance(m1, m2)\n",
        "\n",
        "    def _cluster_distance(self, c1_idxs, c2_idxs, dist_matrix, X):\n",
        "        if self.linkage == 'single': return self._single_linkage(c1_idxs, c2_idxs, dist_matrix)\n",
        "        elif self.linkage == 'complete': return self._complete_linkage(c1_idxs, c2_idxs, dist_matrix)\n",
        "        elif self.linkage == 'ward': return self._ward_linkage(c1_idxs, c2_idxs, X)\n",
        "\n",
        "    def fit(self, X):\n",
        "        n_samples = X.shape[0]\n",
        "        clusters = {i: [i] for i in range(n_samples)}\n",
        "\n",
        "        dist_matrix = None\n",
        "        if self.linkage != 'ward':\n",
        "            dist_matrix = self._compute_distance_matrix(X)\n",
        "\n",
        "        while len(clusters) > self.n_clusters:\n",
        "            min_dist = float('inf')\n",
        "            merge_pair = None\n",
        "            ids = list(clusters.keys())\n",
        "\n",
        "            # Tìm cặp gần nhất\n",
        "            for i in range(len(ids)):\n",
        "                for j in range(i+1, len(ids)):\n",
        "                    id1, id2 = ids[i], ids[j]\n",
        "                    dist = self._cluster_distance(clusters[id1], clusters[id2], dist_matrix, X)\n",
        "                    if dist < min_dist:\n",
        "                        min_dist = dist\n",
        "                        merge_pair = (id1, id2)\n",
        "\n",
        "            # Gộp\n",
        "            c1, c2 = merge_pair\n",
        "            new_id = max(ids) + 1\n",
        "            clusters[new_id] = clusters[c1] + clusters[c2]\n",
        "            del clusters[c1]; del clusters[c2]\n",
        "\n",
        "        # Lưu nhãn cuối cùng\n",
        "        self.labels_ = np.zeros(n_samples, dtype=int)\n",
        "        for idx, (cid, members) in enumerate(clusters.items()):\n",
        "            self.labels_[members] = idx\n",
        "        return self\n",
        "\n",
        "# ============================================================\n",
        "# PHẦN 2: CHUẨN BỊ CHO ENSEMBLE (BASE LEARNERS)\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Load Data\n",
        "    try:\n",
        "        train_path = os.path.join(BASE_PATH, 'data_processed_72.csv')\n",
        "        train_df = pd.read_csv(train_path)\n",
        "        X_train = train_df.drop('Sample_ID', axis=1, errors='ignore').values\n",
        "        print(f\" Đã tải dữ liệu: {X_train.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi: {e}\"); exit()\n",
        "\n",
        "    # 2. Tạo tập hợp các Base Learners (Đa dạng hóa bằng Linkage)\n",
        "    # Lưu ý quan trọng: n_clusters=2 (Chuẩn bài toán ung thư)\n",
        "    linkages = ['single', 'complete', 'ward']\n",
        "    base_models = {}\n",
        "\n",
        "    print(\"\\n ĐANG XÂY DỰNG CÁC BASE LEARNERS...\")\n",
        "\n",
        "    for link in linkages:\n",
        "        print(f\"   -> Training {link.upper()} model...\")\n",
        "        model = HierarchicalClustering(n_clusters=2, linkage=link)\n",
        "        model.fit(X_train)\n",
        "\n",
        "        # Lưu model vào dictionary để dùng cho Ensemble sau này\n",
        "        base_models[link] = model\n",
        "\n",
        "        # Đánh giá nhanh chất lượng từng model con\n",
        "        sil = silhouette_score(X_train, model.labels_)\n",
        "        print(f\"      (Silhouette: {sil:.4f})\")\n",
        "\n",
        "    print(f\"\\n Đã chuẩn bị xong {len(base_models)} mô hình cơ sở cho Ensemble!\")\n",
        "    print(f\"   Danh sách: {list(base_models.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggV9cS2wl_xy"
      },
      "source": [
        "# BASE MODELS - GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5ydZX34dP-rA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "# ============================================================\n",
        "# LOAD DATA (SAU PCA)\n",
        "# ============================================================\n",
        "\n",
        "train_df = pd.read_csv(os.path.join(BASE_PATH, \"data_processed_72.csv\"))\n",
        "\n",
        "X_train = train_df.drop(columns=[\"Sample_ID\"], errors=\"ignore\").values\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# GMM - TỰ CÀI ĐẶT (EM ALGORITHM)\n",
        "# ============================================================\n",
        "\n",
        "class GMM:\n",
        "    def __init__(self, n_components=2, max_iter=100, tol=1e-4):\n",
        "        self.K = n_components\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "\n",
        "    def gaussian(self, X, mean, cov):\n",
        "        d = X.shape[1]\n",
        "        cov_reg = cov + np.eye(d) * 1e-6   # regularization\n",
        "        inv = np.linalg.inv(cov_reg)\n",
        "        det = np.linalg.det(cov_reg)\n",
        "        diff = X - mean\n",
        "        expo = np.sum(diff @ inv * diff, axis=1)\n",
        "        coef = 1.0 / np.sqrt((2 * np.pi) ** d * det)\n",
        "        return coef * np.exp(-0.5 * expo)\n",
        "\n",
        "    def fit(self, X):\n",
        "        n, d = X.shape\n",
        "        rng = np.random.default_rng(42)\n",
        "\n",
        "        # Khởi tạo\n",
        "        self.means = X[rng.choice(n, self.K, replace=False)]\n",
        "        base_cov = np.cov(X.T)\n",
        "        self.covs = np.array([base_cov.copy() for _ in range(self.K)])\n",
        "        self.weights = np.ones(self.K) / self.K\n",
        "\n",
        "        prev_ll = None\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            resp = np.zeros((n, self.K))\n",
        "\n",
        "            # E-step\n",
        "            for k in range(self.K):\n",
        "                resp[:, k] = self.weights[k] * self.gaussian(\n",
        "                    X, self.means[k], self.covs[k]\n",
        "                )\n",
        "\n",
        "            resp_sum = resp.sum(axis=1, keepdims=True)\n",
        "            resp_sum[resp_sum == 0] = 1e-10\n",
        "            resp /= resp_sum\n",
        "\n",
        "            Nk = resp.sum(axis=0)\n",
        "\n",
        "            # M-step\n",
        "            for k in range(self.K):\n",
        "                self.means[k] = np.sum(resp[:, k][:, None] * X, axis=0) / Nk[k]\n",
        "                diff = X - self.means[k]\n",
        "                self.covs[k] = (resp[:, k][:, None] * diff).T @ diff / Nk[k]\n",
        "                self.weights[k] = Nk[k] / n\n",
        "\n",
        "            ll = np.sum(np.log(resp_sum))\n",
        "            if prev_ll is not None and abs(ll - prev_ll) < self.tol:\n",
        "                break\n",
        "            prev_ll = ll\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = np.zeros((X.shape[0], self.K))\n",
        "        for k in range(self.K):\n",
        "            probs[:, k] = self.weights[k] * self.gaussian(\n",
        "                X, self.means[k], self.covs[k]\n",
        "            )\n",
        "        return np.argmax(probs, axis=1)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TRAIN GMM\n",
        "# ============================================================\n",
        "\n",
        "gmm = GMM(n_components=2)\n",
        "gmm.fit(X_train)\n",
        "\n",
        "train_labels = gmm.predict(X_train)\n",
        "test_labels  = gmm.predict(X_test)\n",
        "\n",
        "print(\"Train cluster labels:\", train_labels)\n",
        "print(\"Independent cluster labels:\", test_labels)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# INTERNAL VALIDATION (TRAIN ONLY)\n",
        "# ============================================================\n",
        "\n",
        "sil_score = silhouette_score(X_train, train_labels)\n",
        "db_score  = davies_bouldin_score(X_train, train_labels)\n",
        "\n",
        "print(\"\\nINTERNAL VALIDATION (TRAIN - GMM)\")\n",
        "print(\"Silhouette score:\", round(sil_score, 4))\n",
        "print(\"Davies–Bouldin index:\", round(db_score, 4))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# VISUALIZATION (PCA 2D)\n",
        "# ============================================================\n",
        "\n",
        "X_vis = X_train[:, :2]\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(\n",
        "    X_vis[:, 0], X_vis[:, 1],\n",
        "    c=train_labels,\n",
        "    cmap=\"viridis\",\n",
        "    s=60\n",
        ")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"GMM clustering on TRAIN (PCA space)\")\n",
        "plt.colorbar(label=\"Cluster\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# OUTPUT FOR ENSEMBLE\n",
        "# ============================================================\n",
        "\n",
        "gmm_labels_train = train_labels\n",
        "gmm_labels_test  = test_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdXd_OrwmHKq"
      },
      "source": [
        "# ENSEMBLE CLUSTERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "y1SB32UBTsM5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"--- Đang lấy nhãn từ 3 Base Models ---\")\n",
        "\n",
        "# 1. Base models\n",
        "\n",
        "labels_km, _, _ = kmeans_plusplus(X_all, k=2, random_state=42)\n",
        "\n",
        "bu_model.fit(X_all)\n",
        "labels_bu = bu_model.labels_\n",
        "\n",
        "gmm_model.fit(X_all)\n",
        "labels_gmm = gmm_model.predict(X_all)\n",
        "\n",
        "score = silhouette_score(X_all, ensemble_labels)\n",
        "\n",
        "print(\"Đã thu thập xong nhãn từ K-means++, Bottom-up và GMM.\")\n",
        "\n",
        "# 2. Weights\n",
        "weights = {\n",
        "    \"kmeans\": 0.3,\n",
        "    \"hierarchical\": 0.6,\n",
        "    \"gmm\": 0.1\n",
        "}\n",
        "\n",
        "labels_dict = {\n",
        "    \"kmeans\": labels_km,\n",
        "    \"hierarchical\": labels_bu,\n",
        "    \"gmm\": labels_gmm\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# 3. BUILD WEIGHTED CO-ASSOCIATION MATRIX\n",
        "# ============================================================\n",
        "def build_weighted_co_association_matrix(labels_dict, weights):\n",
        "    model_names = list(labels_dict.keys())\n",
        "    n_samples = len(labels_dict[model_names[0]])\n",
        "\n",
        "    co_matrix = np.zeros((n_samples, n_samples))\n",
        "    total_weight = sum(weights.values())\n",
        "\n",
        "    for name in model_names:\n",
        "        labels = labels_dict[name]\n",
        "        w = weights[name]\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            for j in range(i, n_samples):\n",
        "                if labels[i] == labels[j]:\n",
        "                    co_matrix[i, j] += w\n",
        "                    if i != j:\n",
        "                        co_matrix[j, i] += w\n",
        "\n",
        "    return co_matrix / total_weight\n",
        "\n",
        "\n",
        "print(\"\\n--- Đang xây dựng Weighted Co-association Matrix ---\")\n",
        "co_matrix = build_weighted_co_association_matrix(labels_dict, weights)\n",
        "\n",
        "# ============================================================\n",
        "# 4. FINAL ENSEMBLE CLUSTERING\n",
        "# ============================================================\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "from scipy.spatial.distance import squareform\n",
        "\n",
        "def ensemble_final_step(co_matrix, n_clusters=2):\n",
        "    # Similarity → Distance\n",
        "    dist_matrix = 1 - co_matrix\n",
        "\n",
        "    condensed_dist = squareform(dist_matrix, checks=False)\n",
        "    Z = linkage(condensed_dist, method='average')\n",
        "\n",
        "    final_labels = fcluster(Z, t=n_clusters, criterion='maxclust') - 1\n",
        "    return final_labels, Z\n",
        "\n",
        "\n",
        "print(\"--- Đang thực hiện Final Hierarchical Clustering ---\")\n",
        "ensemble_labels, linkage_matrix = ensemble_final_step(co_matrix, n_clusters=2)\n",
        "\n",
        "# ============================================================\n",
        "# 5. EVALUATION\n",
        "# ============================================================\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "score = silhouette_score(X_train, ensemble_labels)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"KẾT QUẢ ENSEMBLE CLUSTERING (KM++ + BU + GMM)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Final Silhouette Score: {score:.4f}\")\n",
        "print(f\"Nhãn cụm cuối cùng: {ensemble_labels}\")\n",
        "\n",
        "# ============================================================\n",
        "# 6. VISUALIZATION\n",
        "# ============================================================\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(co_matrix, cmap='YlGnBu')\n",
        "plt.title(\"Weighted Co-association Matrix\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "dendrogram(linkage_matrix)\n",
        "plt.title(\"Ensemble Dendrogram\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 7. EXTERNAL EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "df_actual = pd.read_csv(\n",
        "    os.path.join(BASE_PATH, \"actual.csv\")\n",
        ")\n",
        "\n",
        "X_train_patient_ids = list(range(1, 39))  # nếu bệnh nhân train là 1-38\n",
        "\n",
        "# Lọc nhãn train\n",
        "train_actual = df_actual[df_actual['patient'].isin(X_train_patient_ids)]\n",
        "\n",
        "# Map nhãn sang 0/1\n",
        "y_true = train_actual['cancer'].map({'AML': 0, 'ALL': 1}).values\n",
        "\n",
        "# Kiểm tra\n",
        "print(\"Train y_true shape:\", y_true.shape)\n",
        "print(y_true)\n",
        "\n",
        "\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "\n",
        "def purity_score(y_true, y_pred):\n",
        "    # Purity = sum(max count in cluster) / total samples\n",
        "    contingency_matrix = np.zeros((np.max(y_pred)+1, np.max(y_true)+1))\n",
        "    for i in range(len(y_true)):\n",
        "        contingency_matrix[y_pred[i], y_true[i]] += 1\n",
        "    return np.sum(np.max(contingency_matrix, axis=1)) / np.sum(contingency_matrix)\n",
        "\n",
        "# y_true = nhãn thực tế ALL/AML từ actual.csv, dạng integer 0/1\n",
        "# Giả sử mày đã encode nhãn thành 0 = AML, 1 = ALL\n",
        "\n",
        "base_models = {\n",
        "    \"KMeans++\": labels_km,\n",
        "    \"Hierarchical\": labels_bu,\n",
        "    \"GMM\": labels_gmm,\n",
        "    \"Ensemble\": ensemble_labels\n",
        "}\n",
        "\n",
        "print(\"\\n--- EXTERNAL EVALUATION (Train) ---\")\n",
        "for name, labels in base_models.items():\n",
        "    ari = adjusted_rand_score(y_true, labels)\n",
        "    nmi = normalized_mutual_info_score(y_true, labels)\n",
        "    pur = purity_score(y_true, labels)\n",
        "    print(f\"{name}: ARI={ari:.4f}, NMI={nmi:.4f}, Purity={pur:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3eFz43rmJta"
      },
      "source": [
        "#  TEST ON INDEPENDENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Gg13OHCxi7t0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Load test\n",
        "df_test = pd.read_csv(os.path.join(BASE_PATH, \"test_processed.csv\"))\n",
        "X_test = df_test.values\n",
        "# Hoặc nếu có cột Sample_ID:\n",
        "X_test = df_test.drop(columns=[\"Sample_ID\"], errors=\"ignore\").values\n",
        "\n",
        "\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "X_all = np.vstack([X_train, X_test])\n",
        "n_train = X_train.shape[0]\n",
        "n_test = X_test.shape[0]\n",
        "\n",
        "print(\"X_all shape:\", X_all.shape)\n",
        "print(\"\\n--- BASE MODELS trên TRAIN + TEST ---\")\n",
        "\n",
        "# KMeans++\n",
        "labels_km_all, _, _ = kmeans_plusplus(\n",
        "    X_all,\n",
        "    k=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Hierarchical\n",
        "bu_all = HierarchicalClustering(n_clusters=2, linkage='ward')\n",
        "bu_all.fit(X_all)\n",
        "labels_bu_all = bu_all.labels_\n",
        "\n",
        "# GMM\n",
        "gmm_all = GMM(n_components=2)\n",
        "gmm_all.fit(X_all)\n",
        "labels_gmm_all = gmm_all.predict(X_all)\n",
        "\n",
        "print(\"Đã lấy xong nhãn KMeans++, Hierarchical, GMM cho toàn bộ data.\")\n",
        "weights = {\n",
        "    \"kmeans\": 0.3,\n",
        "    \"hierarchical\": 0.6,\n",
        "    \"gmm\": 0.1\n",
        "}\n",
        "\n",
        "labels_dict_all = {\n",
        "    \"kmeans\": labels_km_all,\n",
        "    \"hierarchical\": labels_bu_all,\n",
        "    \"gmm\": labels_gmm_all\n",
        "}\n",
        "\n",
        "def build_weighted_co_association_matrix(labels_dict, weights):\n",
        "    model_names = list(labels_dict.keys())\n",
        "    n_samples = len(labels_dict[model_names[0]])\n",
        "\n",
        "    co_matrix = np.zeros((n_samples, n_samples))\n",
        "    total_weight = sum(weights.values())\n",
        "\n",
        "    for name in model_names:\n",
        "        labels = labels_dict[name]\n",
        "        w = weights[name]\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            for j in range(i, n_samples):\n",
        "                if labels[i] == labels[j]:\n",
        "                    co_matrix[i, j] += w\n",
        "                    if i != j:\n",
        "                        co_matrix[j, i] += w\n",
        "\n",
        "    return co_matrix / total_weight\n",
        "\n",
        "\n",
        "print(\"\\n--- Xây dựng EXTENDED Co-association Matrix ---\")\n",
        "co_matrix_all = build_weighted_co_association_matrix(labels_dict_all, weights)\n",
        "\n",
        "print(\"Co-association shape:\", co_matrix_all.shape)\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "from scipy.spatial.distance import squareform\n",
        "\n",
        "def ensemble_final_step(co_matrix, n_clusters=2):\n",
        "    dist_matrix = 1 - co_matrix\n",
        "    condensed_dist = squareform(dist_matrix, checks=False)\n",
        "    Z = linkage(condensed_dist, method='average')\n",
        "    final_labels = fcluster(Z, t=n_clusters, criterion='maxclust') - 1\n",
        "    return final_labels, Z\n",
        "\n",
        "\n",
        "print(\"\\n--- CONSENSUS CLUSTERING trên EXTENDED MATRIX ---\")\n",
        "ensemble_labels_all, linkage_matrix_all = ensemble_final_step(\n",
        "    co_matrix_all,\n",
        "    n_clusters=2\n",
        ")\n",
        "\n",
        "labels_train_final = ensemble_labels_all[:n_train]\n",
        "labels_test_final  = ensemble_labels_all[n_train:]\n",
        "\n",
        "print(\"Train ensemble labels shape:\", labels_train_final.shape)\n",
        "print(\"Test ensemble labels shape:\", labels_test_final.shape)\n",
        "df_actual = pd.read_csv(os.path.join(BASE_PATH, \"actual.csv\"))\n",
        "\n",
        "# Train: patient 1–38\n",
        "train_actual = df_actual[df_actual['patient'].between(1, 38)]\n",
        "y_train_true = train_actual['cancer'].map({'AML': 0, 'ALL': 1}).values\n",
        "\n",
        "# Test: patient 39–72\n",
        "test_actual = df_actual[df_actual['patient'].between(39, 72)]\n",
        "y_test_true = test_actual['cancer'].map({'AML': 0, 'ALL': 1}).values\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "\n",
        "def purity_score(y_true, y_pred):\n",
        "    contingency = np.zeros((np.max(y_pred)+1, np.max(y_true)+1))\n",
        "    for i in range(len(y_true)):\n",
        "        contingency[y_pred[i], y_true[i]] += 1\n",
        "    return np.sum(np.max(contingency, axis=1)) / np.sum(contingency)\n",
        "\n",
        "\n",
        "print(\"\\n--- EXTERNAL EVALUATION (EXTENDED ENSEMBLE) ---\")\n",
        "\n",
        "print(\"TRAIN:\")\n",
        "print(\"ARI:\", adjusted_rand_score(y_train_true, labels_train_final))\n",
        "print(\"NMI:\", normalized_mutual_info_score(y_train_true, labels_train_final))\n",
        "print(\"Purity:\", purity_score(y_train_true, labels_train_final))\n",
        "\n",
        "print(\"\\nTEST:\")\n",
        "print(\"ARI:\", adjusted_rand_score(y_test_true, labels_test_final))\n",
        "print(\"NMI:\", normalized_mutual_info_score(y_test_true, labels_test_final))\n",
        "print(\"Purity:\", purity_score(y_test_true, labels_test_final))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q28ZAkd9mUVS"
      },
      "source": [
        "#  EVALUATION"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}